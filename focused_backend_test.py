#!/usr/bin/env python3
"""
Focused Backend Test - Tests what we can verify immediately
"""

import requests
import json
import time
import sys

BASE_URL = "https://pdfharvester-1.preview.emergentagent.com"
API_BASE = f"{BASE_URL}/api"

def test_api_responsiveness():
    """Test API responsiveness during active crawl"""
    print("=== Testing API Responsiveness During Crawl ===")
    
    start_time = time.time()
    response_times = []
    
    for i in range(10):
        try:
            start = time.time()
            response = requests.get(f"{API_BASE}/stats", timeout=5)
            end = time.time()
            
            if response.status_code == 200:
                response_time = end - start
                response_times.append(response_time)
                print(f"Stats API call {i+1}: {response_time:.3f}s")
            else:
                print(f"‚ùå Stats API call {i+1} failed: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"‚ùå Stats API call {i+1} failed: {str(e)}")
            return False
        
        time.sleep(2)
    
    avg_response_time = sum(response_times) / len(response_times)
    max_response_time = max(response_times)
    
    print(f"Average response time: {avg_response_time:.3f}s")
    print(f"Max response time: {max_response_time:.3f}s")
    
    if max_response_time > 10:
        print("‚ùå API response times too slow (>10s)")
        return False
    
    print("‚úì API stays responsive during crawl")
    return True

def test_crawl_job_creation_and_progress():
    """Test crawl job creation and progress monitoring"""
    print("\n=== Testing Crawl Job Creation and Progress ===")
    
    # Create job
    job_data = {
        "domain": "https://americanspecialties.com/all-washroom-accessories/",
        "manufacturer_name": "American Specialties (ASI) - Test",
        "product_lines": [],
        "sharepoint_folder": "/DocSyncAgent/Test/ASI-Test",
        "weekly_recrawl": False
    }
    
    try:
        response = requests.post(f"{API_BASE}/crawl-jobs", json=job_data, timeout=30)
        if response.status_code != 200:
            print(f"‚ùå Failed to create crawl job: {response.status_code}")
            return False, None
        
        job = response.json()
        job_id = job['id']
        print(f"‚úì Created crawl job: {job_id}")
        
        # Monitor for initial progress
        for i in range(20):  # Monitor for 1 minute
            try:
                response = requests.get(f"{API_BASE}/crawl-jobs/{job_id}", timeout=10)
                if response.status_code == 200:
                    current_job = response.json()
                    status = current_job.get('status')
                    pdfs_found = current_job.get('total_pdfs_found', 0)
                    
                    print(f"Status: {status}, PDFs found: {pdfs_found}")
                    
                    # If we see progress (status change or PDFs found), that's good
                    if status != 'pending' or pdfs_found > 0:
                        print("‚úì Job shows progress")
                        return True, job_id
                        
                else:
                    print(f"‚ùå Failed to get job status: {response.status_code}")
                    return False, job_id
                    
            except Exception as e:
                print(f"‚ùå Error getting job status: {str(e)}")
                return False, job_id
            
            time.sleep(3)
        
        print("‚ö†Ô∏è No progress observed in 1 minute")
        return False, job_id
        
    except Exception as e:
        print(f"‚ùå Error creating crawl job: {str(e)}")
        return False, None

def test_job_cancellation(job_id):
    """Test job cancellation"""
    print(f"\n=== Testing Job Cancellation for {job_id} ===")
    
    if not job_id:
        print("‚ùå No job ID provided for cancellation test")
        return False
    
    try:
        # Cancel the job
        response = requests.post(f"{API_BASE}/crawl-jobs/{job_id}/cancel", timeout=10)
        if response.status_code != 200:
            print(f"‚ùå Failed to cancel job: {response.status_code}")
            return False
        
        print("‚úì Cancel request sent")
        
        # Verify cancellation
        time.sleep(3)
        response = requests.get(f"{API_BASE}/crawl-jobs/{job_id}", timeout=10)
        if response.status_code == 200:
            job = response.json()
            if job.get('status') == 'cancelled':
                print("‚úì Job successfully cancelled")
                return True
            else:
                print(f"‚ùå Job status is {job.get('status')}, expected 'cancelled'")
                return False
        else:
            print(f"‚ùå Failed to verify cancellation: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"‚ùå Error during cancellation test: {str(e)}")
        return False

def check_existing_crawl_results():
    """Check if there are any existing crawl results we can analyze"""
    print("\n=== Checking Existing Crawl Results ===")
    
    try:
        # Get all jobs
        response = requests.get(f"{API_BASE}/crawl-jobs", timeout=10)
        if response.status_code != 200:
            print(f"‚ùå Failed to get jobs: {response.status_code}")
            return False
        
        jobs = response.json()
        
        # Look for ASI jobs with PDFs found
        asi_jobs_with_pdfs = [
            job for job in jobs 
            if "American Specialties" in job.get('manufacturer_name', '') 
            and job.get('total_pdfs_found', 0) > 0
        ]
        
        if not asi_jobs_with_pdfs:
            print("No ASI jobs with PDFs found")
            return False
        
        print(f"Found {len(asi_jobs_with_pdfs)} ASI jobs with PDFs")
        
        # Check the most recent one
        job = asi_jobs_with_pdfs[0]
        job_id = job['id']
        
        print(f"Checking job {job_id}:")
        print(f"  Status: {job['status']}")
        print(f"  PDFs found: {job['total_pdfs_found']}")
        print(f"  PDFs classified: {job['total_pdfs_classified']}")
        print(f"  PDFs uploaded: {job['total_pdfs_uploaded']}")
        
        # This confirms the crawling part is working
        if job['total_pdfs_found'] > 0:
            print("‚úì Crawling functionality is working (PDFs discovered)")
            
            # Check if we can get PDF records (even if empty due to classification issues)
            response = requests.get(f"{API_BASE}/crawl-jobs/{job_id}/pdfs", timeout=10)
            if response.status_code == 200:
                print("‚úì PDF records endpoint is accessible")
                return True
            else:
                print(f"‚ùå PDF records endpoint failed: {response.status_code}")
                return False
        else:
            print("‚ùå No PDFs found in existing jobs")
            return False
            
    except Exception as e:
        print(f"‚ùå Error checking existing results: {str(e)}")
        return False

def main():
    """Run focused backend tests"""
    print("=== PDF DocSync Agent - Focused Backend Tests ===\n")
    
    results = {}
    
    # Test 1: API Responsiveness
    results['api_responsive'] = test_api_responsiveness()
    
    # Test 2: Check existing crawl results
    results['existing_results'] = check_existing_crawl_results()
    
    # Test 3: Create new job and monitor progress
    progress_success, job_id = test_crawl_job_creation_and_progress()
    results['job_creation'] = progress_success
    
    # Test 4: Cancellation
    results['cancellation'] = test_job_cancellation(job_id)
    
    # Summary
    print(f"\n=== TEST SUMMARY ===")
    print(f"API Responsiveness: {'PASSED' if results['api_responsive'] else 'FAILED'}")
    print(f"Existing Results Check: {'PASSED' if results['existing_results'] else 'FAILED'}")
    print(f"Job Creation & Progress: {'PASSED' if results['job_creation'] else 'FAILED'}")
    print(f"Job Cancellation: {'PASSED' if results['cancellation'] else 'FAILED'}")
    
    passed_tests = sum(1 for result in results.values() if result)
    total_tests = len(results)
    
    print(f"\nPassed: {passed_tests}/{total_tests} tests")
    
    if passed_tests >= 3:  # At least 3 out of 4 tests should pass
        print("üéâ CORE FUNCTIONALITY WORKING")
        return True
    else:
        print("‚ùå CORE FUNCTIONALITY ISSUES")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)